{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os, sys\n",
    "from glob import glob\n",
    "\n",
    "# core nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# gensim magic\n",
    "import gensim\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(raw_text):\n",
    "    # tokenize\n",
    "    tokens = word_tokenize(raw_text)    \n",
    "\n",
    "    # drop to lowercase\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "        \n",
    "    # *step two* (default): remove non-alpha characters,\n",
    "    # punctuation, and as many other \"noise\" elements as\n",
    "    # possible. If dealing with a single character word,    \n",
    "    # drop non-alphabetical characters. This will remove \n",
    "    # most punctuation but preserve many words containing\n",
    "    # marks such as the '-' in 'self-emancipated'\n",
    "\n",
    "    tmp_text=list()\n",
    "\n",
    "    for word in tokens:\n",
    "        if len(word) == 1:\n",
    "            if word.isalpha == True:\n",
    "                tmp_text.append(word)\n",
    "        else:\n",
    "             tmp_text.append(word)           \n",
    "    tokens = tmp_text\n",
    "\n",
    "    # now remove leading and trailing quotation marks,      \n",
    "    # hyphens and  dashes\n",
    "    tmp_text=list()\n",
    "    drop_list = ['“','\"','”','-','—']\n",
    "    for i, word in enumerate(tokens):\n",
    "        if word[0] in drop_list:\n",
    "            word = word[1:]\n",
    "        if word[-1:] in drop_list:\n",
    "            word = word[:-1]\n",
    "        \n",
    "        word = word.replace(\"gyftis\", \"gifts\")\n",
    "        word = word.replace(\"gether\", \"gather\")\n",
    "        word = word.replace(\"spirituall\", \"spiritual\")\n",
    "        word = word.replace(\"feythfull\", \"faith\")\n",
    "        word = word.replace(\"wytnes\", \"witness\")\n",
    "        word = word.replace(\"almes\", \"alms\")\n",
    "        word = word.replace(\"desyre\", \"desire\")\n",
    "        word = word.replace(\"selfe\", \"self\")\n",
    "        word = word.replace(\"saffely\", \"safely\")\n",
    "        word = word.replace(\"realme\", \"realm\")\n",
    "        word = word.replace(\"acte\", \"act\")\n",
    "        word = word.replace(\"fourme\", \"form\")\n",
    "        word = word.replace(\"subiectes\", \"subjects\")\n",
    "        word = word.replace(\"theyr\", \"their\")\n",
    "        word = word.replace(\"kynde\", \"kind\")\n",
    "        word = word.replace(\"kynge\", \"king\")\n",
    "        word = word.replace(\"kyndes\", \"kinds\")\n",
    "        word = word.replace(\"vpon\", \"unto\")\n",
    "        word = word.replace(\"purueyours\", \"purveyors\")\n",
    "        word = word.replace(\"highnes\", \"highness\")\n",
    "        word = word.replace(\"euery\", \"every\")\n",
    "        word = word.replace(\"quene\", \"queen\")\n",
    "        word = word.replace(\"quenes\", \"queens\")\n",
    "        word = word.replace(\"whiche\", \"which\")\n",
    "        word = word.replace(\"bloude\", \"blood\")\n",
    "        word = word.replace(\"soueraine\", \"sovereign\")\n",
    "        \n",
    "        # catch any zero-length words remaining\n",
    "        if len(word) > 0:\n",
    "            tmp_text.append(word)\n",
    "        \n",
    "    return(tmp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_vectors(sentences):\n",
    "    # source documents\n",
    "    # dimension of feature vectors \n",
    "    # max distance   \n",
    "    # number of times a word must appear to be included in vocab\n",
    "    # for parallelization\n",
    "\n",
    "    print(\"starting training...\")\n",
    "    model = gensim.models.Word2Vec(\n",
    "        sentences, \n",
    "        sg=0,           # sg=1 is use skip-gram, sg=0 is cbow \n",
    "        size=200,        \n",
    "        window=15,     \n",
    "        min_count=2,    # increase to limit vocab and find fewer rare words\n",
    "        workers=10,     \n",
    "        iter=10)\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# henry_VII 1486 - 1509\n",
    "#  ----------------------\n",
    "# henry_VII 1510 - 1547\n",
    "# edward_VI 1548 - 1553\n",
    "# mary_I 1554 - 1558\n",
    "# elizabeth_I 1559 - 1603\n",
    "# james_I - 1604 - 1625\n",
    "# charles_I - 1626 - 1649\n",
    "#  ----------------------\n",
    "# oliver_cromwell - 1650 - 1660\n",
    "# charles_II > 1660"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "doc_crown = dict()\n",
    "for x in sorted(glob(\"../texts/eebo/eebo-year*\")):\n",
    "    year = os.path.basename(x).split('.')[0]\n",
    "    year = int(year.split('-')[2])\n",
    "    if year > 1510 and year < 1547:\n",
    "        doc_crown[x] = \"henry_VII\"\n",
    "    if year > 1548 and year < 1553:\n",
    "        doc_crown[x] = \"edward_VI\"   \n",
    "    if year > 1554 and year < 1558:\n",
    "        doc_crown[x] = \"mary_I\"   \n",
    "    if year > 1559 and year < 1603:\n",
    "        doc_crown[x] = \"elizabeth_I\"   \n",
    "    if year > 1604 and year < 1625:\n",
    "        doc_crown[x] = \"james_I\"         \n",
    "    if year > 1626:\n",
    "        doc_crown[x] = \"charles_I\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting: elizabeth_I\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "segmenting...\n",
      "starting training...\n",
      "saving output\n",
      "starting: james_I\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "segmenting...\n",
      "starting training...\n",
      "saving output\n",
      "starting: mary_I\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "segmenting...\n",
      "starting training...\n",
      "saving output\n",
      "starting: edward_VI\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "segmenting...\n",
      "starting training...\n",
      "saving output\n",
      "starting: henry_VII\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "segmenting...\n",
      "starting training...\n",
      "saving output\n",
      "starting: charles_I\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "segmenting...\n",
      "starting training...\n",
      "saving output\n"
     ]
    }
   ],
   "source": [
    "# begin constructing model\n",
    "import gzip\n",
    "import gc \n",
    "\n",
    "# create dict holder for models\n",
    "eebo_models = dict()\n",
    "\n",
    "# iterate through the sovereigns\n",
    "for crown in set(doc_crown.values()):\n",
    "    tokens = list()\n",
    "    print(\"starting: {0}\".format(crown))\n",
    "    \n",
    "    # extract all files for this crown\n",
    "    for k, v in doc_crown.items():\n",
    "        if v == crown:\n",
    "            print(\"loading gzipped texts...\")\n",
    "            raw_text = gzip.open(k,'rt').read()\n",
    "            \n",
    "            print(\"preprocessing...\")\n",
    "            tokens = tokens + preprocess(raw_text)\n",
    "\n",
    "            # free memory\n",
    "            del raw_text\n",
    "            gc.collect()\n",
    "\n",
    "    # simulate documents\n",
    "    print(\"segmenting...\")\n",
    "    sample_sentences = list()\n",
    "    segment_length = int(len(tokens)/1000)\n",
    "    \n",
    "    for j in range(1000):\n",
    "        segment = tokens[segment_length*j:segment_length*(j+1)]\n",
    "        sample_sentences.append(segment)\n",
    "        \n",
    "    # free up memory\n",
    "    del tokens\n",
    "    gc.collect()\n",
    "\n",
    "    # train model\n",
    "    eebo_models[crown] = train_vectors(sample_sentences)\n",
    "    \n",
    "    # save model\n",
    "    print(\"saving output\")\n",
    "    fp = open(\"../models/eebo-\" + crown + \".w2v\",'wb')\n",
    "    eebo_models[crown].wv.save(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
