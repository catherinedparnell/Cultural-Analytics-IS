{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os, sys\n",
    "from glob import glob\n",
    "\n",
    "# core nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# gensim magic\n",
    "import gensim\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_text):\n",
    "    # tokenize\n",
    "    tokens = word_tokenize(raw_text)    \n",
    "\n",
    "    # drop to lowercase\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "        \n",
    "    # *step two* (default): remove non-alpha characters,\n",
    "    # punctuation, and as many other \"noise\" elements as\n",
    "    # possible. If dealing with a single character word,    \n",
    "    # drop non-alphabetical characters. This will remove \n",
    "    # most punctuation but preserve many words containing\n",
    "    # marks such as the '-' in 'self-emancipated'\n",
    "\n",
    "    tmp_text=list()\n",
    "\n",
    "    for word in tokens:\n",
    "        if len(word) == 1:\n",
    "            if word.isalpha == True:\n",
    "                tmp_text.append(word)\n",
    "        else:\n",
    "             tmp_text.append(word)           \n",
    "    tokens = tmp_text\n",
    "\n",
    "    # now remove leading and trailing quotation marks,      \n",
    "    # hyphens and  dashes\n",
    "    tmp_text=list()\n",
    "    drop_list = ['“','\"','”','-','—']\n",
    "    for word in tokens:\n",
    "        if word[0] in drop_list:\n",
    "            word = word[1:]\n",
    "        if word[-1:] in drop_list:\n",
    "            word = word[:-1]\n",
    "\n",
    "        # catch any zero-length words remaining\n",
    "        if len(word) > 0:\n",
    "            tmp_text.append(word)\n",
    "        \n",
    "    return(tmp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vectors(sentences):\n",
    "    # source documents\n",
    "    # dimension of feature vectors \n",
    "    # max distance   \n",
    "    # number of times a word must appear to be included in vocab\n",
    "    # for parallelization\n",
    "\n",
    "    print(\"starting training...\")\n",
    "    model = gensim.models.Word2Vec(\n",
    "        sentences, \n",
    "        sg=0,           # sg=1 is use skip-gram, sg=0 is cbow \n",
    "        size=200,        \n",
    "        window=15,     \n",
    "        min_count=2,    # increase to limit vocab and find fewer rare words\n",
    "        workers=10,     \n",
    "        iter=10)\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting: eebo-1520-1529\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "segmenting...\n",
      "starting training...\n"
     ]
    }
   ],
   "source": [
    "# begin constructing model\n",
    "import gzip\n",
    "import gc \n",
    "\n",
    "eebo_models = dict()\n",
    "input_data = [\"../texts/eebo/eebo-1520-1529.txt.gz\",\n",
    "              \"../texts/eebo/eebo-1530-1539.txt.gz\",\n",
    "              \"../texts/eebo/eebo-1540-1549.txt.gz\",\n",
    "              \"../texts/eebo/eebo-1550-1559.txt.gz\",\n",
    "              \"../texts/eebo/eebo-1560-1569.txt.gz\",\n",
    "              \"../texts/eebo/eebo-1570-1579.txt.gz\",\n",
    "              \"../texts/eebo/eebo-1580-1589.txt.gz\",\n",
    "              \"../texts/eebo/eebo-1590-1599.txt.gz\",\n",
    "              \"../texts/eebo/eebo-1600-1609.txt.gz\",\n",
    "              \"../texts/eebo/eebo-1610-1619.txt.gz\",\n",
    "              \"../texts/eebo/eebo-1620-1629.txt.gz\",\n",
    "              \"../texts/eebo/eebo-1630-1639.txt.gz\"]\n",
    "\n",
    "\n",
    "#for fp in glob(\"../texts/eebo/eebo-*.gz\"):\n",
    "for fp in input_data:\n",
    "    \n",
    "    model_name = os.path.basename(fp).split(\".\")[0]\n",
    "    print(\"starting: {0}\".format(model_name))\n",
    "\n",
    "    print(\"loading gzipped texts...\")\n",
    "    raw_text = gzip.open(fp,'rt').read()\n",
    "    \n",
    "    print(\"preprocessing...\")\n",
    "    tokens = preprocess(raw_text)\n",
    "\n",
    "    # simulate documents\n",
    "    print(\"segmenting...\")\n",
    "    sample_sentences = list()\n",
    "    segment_length = int(len(tokens)/1000)\n",
    "    \n",
    "    for j in range(1000):\n",
    "        segment = tokens[segment_length*j:segment_length*(j+1)]\n",
    "        sample_sentences.append(segment)\n",
    "        \n",
    "    # free up memory\n",
    "    del raw_text\n",
    "    gc.collect()\n",
    "\n",
    "    # train model\n",
    "    eebo_models[model_name] = train_vectors(sample_sentences)\n",
    "    \n",
    "    # save model\n",
    "    print(\"saving output\")\n",
    "    fp = open(\"../models/\" + model_name + \".w2v\",'wb')\n",
    "    eebo_models[model_name].wv.save(fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
