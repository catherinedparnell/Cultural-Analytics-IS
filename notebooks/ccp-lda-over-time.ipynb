{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import os, sys\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import gc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_text):\n",
    "    # tokenize\n",
    "    tokens = word_tokenize(raw_text)    \n",
    "\n",
    "    # drop to lowercase\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "        \n",
    "    # *step two* (default): remove non-alpha characters,\n",
    "    # punctuation, and as many other \"noise\" elements as\n",
    "    # possible. If dealing with a single character word,    \n",
    "    # drop non-alphabetical characters. This will remove \n",
    "    # most punctuation but preserve many words containing\n",
    "    # marks such as the '-' in 'self-emancipated'\n",
    "\n",
    "    tmp_text=list()\n",
    "\n",
    "    for word in tokens:\n",
    "        if len(word) == 1:\n",
    "            if word.isalpha == True:\n",
    "                tmp_text.append(word)\n",
    "        else:\n",
    "             tmp_text.append(word)           \n",
    "    tokens = tmp_text\n",
    "\n",
    "    # now remove leading and trailing quotation marks,      \n",
    "    # hyphens and  dashes\n",
    "    tmp_text=list()\n",
    "    drop_list = ['“','\"','”','-','—']\n",
    "    for i, word in enumerate(tokens):\n",
    "        if word[0] in drop_list:\n",
    "            word = word[1:]\n",
    "        if word[-1:] in drop_list:\n",
    "            word = word[:-1]\n",
    "        \n",
    "        word = word.replace(\"gyftis\", \"gifts\")\n",
    "        word = word.replace(\"gether\", \"gather\")\n",
    "        word = word.replace(\"spirituall\", \"spiritual\")\n",
    "        word = word.replace(\"feythfull\", \"faith\")\n",
    "        word = word.replace(\"wytnes\", \"witness\")\n",
    "        word = word.replace(\"almes\", \"alms\")\n",
    "        word = word.replace(\"desyre\", \"desire\")\n",
    "        word = word.replace(\"selfe\", \"self\")\n",
    "        word = word.replace(\"saffely\", \"safely\")\n",
    "        word = word.replace(\"realme\", \"realm\")\n",
    "        word = word.replace(\"acte\", \"act\")\n",
    "        word = word.replace(\"fourme\", \"form\")\n",
    "        word = word.replace(\"subiectes\", \"subjects\")\n",
    "        word = word.replace(\"theyr\", \"their\")\n",
    "        word = word.replace(\"kynde\", \"kind\")\n",
    "        word = word.replace(\"kynge\", \"king\")\n",
    "        word = word.replace(\"kyndes\", \"kinds\")\n",
    "        word = word.replace(\"vpon\", \"unto\")\n",
    "        word = word.replace(\"purueyours\", \"purveyors\")\n",
    "        word = word.replace(\"highnes\", \"highness\")\n",
    "        word = word.replace(\"euery\", \"every\")\n",
    "        word = word.replace(\"quene\", \"queen\")\n",
    "        word = word.replace(\"quenes\", \"queens\")\n",
    "        word = word.replace(\"whiche\", \"which\")\n",
    "        word = word.replace(\"bloude\", \"blood\")\n",
    "        word = word.replace(\"soueraine\", \"sovereign\")\n",
    "        \n",
    "        if word.isdigit():\n",
    "            word = \"\"\n",
    "        \n",
    "        # catch any zero-length words remaining\n",
    "        if len(word) > 0:\n",
    "            tmp_text.append(word)\n",
    "        \n",
    "    return(tmp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting: henry\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "segmenting...\n",
      "read 1000 documents with 1713 vocabulary\n",
      "starting: edward\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "segmenting...\n",
      "read 1000 documents with 6313 vocabulary\n",
      "starting: mary\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "segmenting...\n",
      "read 1000 documents with 442 vocabulary\n",
      "starting: elizabeth\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "segmenting...\n",
      "read 1000 documents with 5771 vocabulary\n",
      "starting: james\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "segmenting...\n",
      "read 1000 documents with 9247 vocabulary\n",
      "starting: charles\n",
      "loading gzipped texts...\n",
      "preprocessing...\n",
      "segmenting...\n",
      "read 1000 documents with 1509 vocabulary\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "ccp_models = dict()\n",
    "\n",
    "input_texts = [\"../texts/henry/henry.tar.gz\",\n",
    "              \"../texts/edward/edward.tar.gz\",\n",
    "              \"../texts/mary/mary.tar.gz\",\n",
    "              \"../texts/elizabeth/elizabeth.tar.gz\",\n",
    "              \"../texts/james/james.tar.gz\",\n",
    "              \"../texts/charles/charles.tar.gz\"]\n",
    "\n",
    "# setup vectorizer and process text\n",
    "\n",
    "for fp in input_texts:\n",
    "    \n",
    "    model_name = os.path.basename(fp).split(\".\")[0]\n",
    "    print(\"starting: {0}\".format(model_name))\n",
    "\n",
    "    print(\"loading gzipped texts...\")\n",
    "    raw_text = gzip.open(fp,'rt').read()\n",
    "    \n",
    "    print(\"preprocessing...\")\n",
    "    tokens = preprocess(raw_text)\n",
    "\n",
    "    # simulate documents\n",
    "    print(\"segmenting...\")\n",
    "    collection = list()\n",
    "    segment_length = int(len(tokens)/1000)\n",
    "    \n",
    "    for j in range(1000):\n",
    "        segment = tokens[segment_length*j:segment_length*(j+1)]\n",
    "        collection.append(' '.join(segment))\n",
    "        \n",
    "    # free up memory\n",
    "    del raw_text\n",
    "    gc.collect()\n",
    "    \n",
    "    vec = CountVectorizer(input='content',\n",
    "                      min_df=2,\n",
    "                      stop_words = [\",\",\"the\",\"and\",\"of\",\"or\",\"to\",\"in\",\"shall\",\"be\",\"that\",\"any\",\"by\",\".\",\n",
    "              \"such\",\"as\",\"this\",\"for\",\"same\",\"all\",\"said\",\"other\",\"'s\",\";\",\n",
    "              \"her\",\"is\",\"every\",\"[\",\"]\",\"they\",\"within\", \"our\", \"not\", \"so\",\n",
    "              \"made\", \"no\", \"then\", \":\", \"do\", \"from\", \"if\", \"it\", \"which\", \"at\", \"with\",\n",
    "             \"thereof\",\"upon\", \"a\", \"because\", \"used\", \"some\", \"but\", \"aforesaid\", \"also\",\n",
    "             \")\",\"(\", \"what\", \"&\", \"may\", \"are\", \"their\", \"them\", \"sayde\", \"suche\", \"shalbe\", \"anye\", \"sayd\",\n",
    "             \"thesaid\", \"/\", \"...\", \"/\", \"either\"],\n",
    "                      lowercase=True)\n",
    "\n",
    "    # train model (LDA)\n",
    "    counts = vec.fit_transform(collection)\n",
    "    dc, vc = counts.shape\n",
    "    print(\"read {0} documents with {1} vocabulary\".format(dc,vc))\n",
    "    \n",
    "    # Build the LDA Model\n",
    "    # n_components = number of topics to extract (if topics are too similar, extract more)\n",
    " \n",
    "    ccp_model = LatentDirichletAllocation(n_components=2,\n",
    "                                            max_iter=5,\n",
    "                                            learning_method='batch',\n",
    "                                            random_state=1)\n",
    "    ccp_models[model_name] = [ccp_model, counts, vec]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "henry:\n",
      "\n",
      "Topic #0:\n",
      "he his king my have was you on should has him had these things book henry thomist us against when scripture will one let who words your would luther does defender most how god christ lies argument man were more than here we now great can himself says world time papacy church therefore me after its sacraments first grace very whole water out where own concerning set truth into being an see good there like royal over england place come believe lord matter saint mass ye those part once think sacred up worthy reader thomas write am why foolish unto \n",
      "\n",
      "Topic #1:\n",
      "have he faith me will men who god christ has we can say nothing thomist church bread only even my there king one make therefore sacrament work right words was been word thing body must nor does should were against him these mass now scripture man henry since good an more lord use authority his paul than alone us when on scriptures articles prove let into here own those see could concerning how sacrifice saying way papists time ye believe am people holy nay although know its necessary pope did unless new answer satan thomists would written things without done \n",
      "\n",
      "edward:\n",
      "\n",
      "Topic #0:\n",
      "ther master was ys day armes men my after lord cam blake grett ij london on unto iiij gownes ser he wher sant iij bered have clarkes dosen dyd dener mony mare standard goodly chyrche had blank mornars women skochyons whyt grace mad armur hangyd odur whent gold sword his there syngyng haroldes here god gret garter pryche viij quen pore kyng we good resident xx cott alle westmynster red man you thay your syd evere yerle ye one rod vj were hym althermen xij baner furst harold lade nodur june velvett borne bowt bare cottes sam corse plase sermon \n",
      "\n",
      "Topic #1:\n",
      "was day lord he his master on sir ther london my after ij had grett john iiij bered dyd armes funeral thomas sam see ys sant st unto dosen quen men ibid one diary ser vj man blank wyche died ms were lady wher mony torchys grace pryche august edward she william iij who king xij marche vol resident an buried dener skochyons july queen have lade september henry duke son knyght there been mare kyng towre cam masse branchys earl stowe odur viij mad october november church will has tapurs bysshope whyt wyff january ded hangyd whent sermon dyvers \n",
      "\n",
      "mary:\n",
      "\n",
      "Topic #0:\n",
      "god 000000 persons subjects kind person one after highness king take most lambes queens deputies wethers lawes beues we have 00catherineparnell ustar staff tyme salte enactd his true should lord well calues fishe ought muche day prouided jackman further religion swyne deputie hyghnes year estatutes counties contayned before aucthoritie act queen time late maiesties yf therefore al were dominions being punishment session constable obedient moste kyng forth out schoolmasters past cowlyn first diuerse countie 000024 named seuerall heretofore set done wherof only make moost vndertakers calfes men toward feast spiritual hym man where therein forfeyture soeuer persones acts daye sort \n",
      "\n",
      "Topic #1:\n",
      "good commission realm commissions unto haue chease butter there grayne kinds vnto purveyors bakon gese capons 000644 txt vs he great my aforesayde connies hennes everye pigges ye people an taken graunted parliament grace come queen 000765 both without hath you john subjects 000024 aucthorised thys present many matters shal those on act constables false therof much whatsoeuer persone maiestie intituled beynge your written saue duty just euyll takers have might expressed therfore vertue officers agaynst peace next vessels came wish barrelles holden brefes englande dokettes lewde order lorde maye understand him churches sessions here reygne touching life fyrste nexte \n",
      "\n",
      "elizabeth:\n",
      "\n",
      "Topic #0:\n",
      "we god have he his us thy christ unto men was these holy him church were lorde hath thou one man yet there my lord say when father who will people good ye you me had did should into only most nor faith things let many more without own how can jesus ever life out saith amen against now thee doth come before up an make name himself might world shal than been after your rome very being time minister al truth fathers gospel great therefore mercy those grace whom true neither would cause ought through even called see away \n",
      "\n",
      "Topic #1:\n",
      "kl lesson id prayer after first john prid day acts kal person second days act one unto idus april common easter luke his evening kings realm added time saint sunday hath before authority eccles appointed book psalms offence reg enactd next lessons mary majesty cor term persons sol notes deut thirty queen gen there highnesss kalend your service form shal moring nonas where etc trinity england he convicted isaiah exod matt march twenty parliament 12 use morning provided bee place esd isa on was hereafter apostle baptist holy edward dominions dayes ascension james king peter proper st wales table contrary \n",
      "\n",
      "james:\n",
      "\n",
      "Topic #0:\n",
      "he his unto was him lord king were israel had son came when children out house on went there people up sons god into one men 11 before 10 13 thou an 12 day 14 15 david against saying ye two now man land after these have did over took hundred 16 come offering city moses jerusalem 17 go hand me 18 great even thousand thee brought we us 19 21 put judah according sent 20 priest 23 days father thy 24 my will 25 three 22 26 you behold she about let down shalt 27 pass side spake place \n",
      "\n",
      "Topic #1:\n",
      "he lord unto his thou thy my him will me god thee have ye man you hath when out was we up come 10 your there 11 12 people 14 16 shalt into 13 let things saith 15 us one 18 hast 19 therefore before 22 an 20 against land day 17 go 21 men say behold earth thine hand even now 23 jesus who neither on israel were am down heart 24 she make house know good away son give 25 father name 26 these like great came had forth nor among saying yet more 27 mine whom take \n",
      "\n",
      "charles:\n",
      "\n",
      "Topic #0:\n",
      "his god he we thy him christ holy us unto church have most father one will sin who hath son jesus sins thee yet own word good body therefore life lord those things being world man these people spirit according come himself whom grace through power both more was after almighty an receive your into nor blood can communion gen death only men themselves mercy without when there law before works thereunto you up were under did priest glory day truly take lawful truth bread make against faith kingdom doth divine persons given peace on hearts love sacrament whole although \n",
      "\n",
      "Topic #1:\n",
      "rom ii john cor 13 10 11 matt ps 15 12 acts heb 17 14 god eph 16 lord 19 20 18 christ gal 22 23 unto isa pet tim holy 24 28 luke 26 21 25 iii faith col priest chapter man time gen have 30 iv hath rev only give 27 there thess called 32 she grace deut under people job spirit exod was say men gospel my 31 jer 33 communion his you mark glory jesus through 34 james life 29 sacrament vi worship appointed testament thou kings name new doth phil will thy him covenant sam \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ccp_model_name in ccp_models:\n",
    "    # get fitted data and transformed matrix\n",
    "    ccp_data = ccp_models[ccp_model_name][0].fit(ccp_models[ccp_model_name][1])\n",
    "\n",
    "    # extract the features to a simple list\n",
    "    feature_names = ccp_models[ccp_model_name][2].get_feature_names()\n",
    "\n",
    "    # how many words do we want to extract for each topic?\n",
    "    n_words = 100\n",
    "    \n",
    "    print(ccp_model_name+\":\\n\")\n",
    "    # now produce topics\n",
    "    for topic_idx, topic in enumerate(ccp_models[ccp_model_name][0].components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        for i in topic.argsort()[:-n_words - 1:-1]:\n",
    "            print(\"{0} \".format(feature_names[i]),end=\"\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
