{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import nltk\n",
    "import sklearn\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define period boundaries\n",
    "date_min = 1520\n",
    "date_max = 1640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# search EEBO TCP texts for texts published between period markers\n",
    "metadata=list()\n",
    "   \n",
    "eebo_root=\"~/eebo-tcp/eebo-tcp-A/\"\n",
    "all_text = list()\n",
    "\n",
    "for text in os.listdir(eebo_root):\n",
    "        path = eebo_root + \"/\" + text\n",
    "        xml_object = etree.parse(path)\n",
    "        pubdate = xml_object.findall(\".//{http://www.tei-c.org/ns/1.0}date\")[0]\n",
    "        \n",
    "        # extract date text\n",
    "        pubdate = pubdate.xpath(\"text()\")[0]\n",
    "        \n",
    "        # keep if four digits, convert to int\n",
    "        if len(pubdate) == 4:\n",
    "            try:\n",
    "                pubdate = int(pubdate)\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if pubdate > date_min and pubdate < date_max:\n",
    "            \n",
    "            # check for language and keep only English\n",
    "            if xml_object.findall(\".//{http://www.tei-c.org/ns/1.0}language\")[0].xpath(\"text()\")[0] == \"eng\":\n",
    "                \n",
    "                # add to metadata\n",
    "                metadata.append([pubdate,text])\n",
    "            \n",
    "                # extract text\n",
    "                text_object = xml_object.findall('.//{http://www.tei-c.org/ns/1.0}text')\n",
    "                text_object = text_object[0].xpath(\".//text()\")\n",
    "                text_object=' '.join(text_object)\n",
    "            \n",
    "                # add to text archive\n",
    "                all_text.append(text_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now save text and metadata\n",
    "import pickle\n",
    "fp = open('eebo-tcp-a_engl_data.pkl','wb')\n",
    "pickle.dump(metadata,fp)\n",
    "pickle.dump(all_text,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now get ready for model\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "eebo_documents = list()\n",
    "for raw_text in all_text:\n",
    "    tokens = word_tokenize(raw_text)\n",
    "    \n",
    "    # make lowercase \n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    # *step two* (default): remove non-alpha characters,\n",
    "    # punctuation, and as many other \"noise\" elements as\n",
    "    # possible. If dealing with a single character word,    \n",
    "    # drop non-alphabetical characters. This will remove \n",
    "    # most punctuation but preserve many words containing\n",
    "    # marks such as the '-' in 'self-emancipated'\n",
    "\n",
    "    tmp_text=list()\n",
    "    for word in tokens:\n",
    "        if len(word) == 1:\n",
    "            if word.isalpha == True:\n",
    "                tmp_text.append(word)\n",
    "        else:\n",
    "            tmp_text.append(word)           \n",
    "    tokens = tmp_text\n",
    "\n",
    "    # now remove leading and trailing quotation marks,      \n",
    "    # hyphens and  dashes\n",
    "    tmp_text=list()\n",
    "    drop_list = ['“','\"','”','-','—']\n",
    "    for word in tokens:\n",
    "        if word[0] in drop_list:\n",
    "            word = word[1:]\n",
    "        if word[-1:] in drop_list:\n",
    "            word = word[:-1]\n",
    "\n",
    "        # catch any zero-length words remaining\n",
    "        if len(word) > 0:\n",
    "            tmp_text.append(word)\n",
    "    tokens = tmp_text\n",
    "    \n",
    "    eebo_documents.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save a little memory\n",
    "del all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# source documents\n",
    "# dimension of feature vectors \n",
    "# max distance   \n",
    "# number of times a word must appear to be included in vocab\n",
    "# for parallelization\n",
    "\n",
    "eebo_model = gensim.models.Word2Vec(\n",
    "    eebo_documents, \n",
    "    sg=1,           # sg=1 is use skip-gram, sg=0 is cbow \n",
    "    size=200,        \n",
    "    window=10,     \n",
    "    min_count=2,    \n",
    "    workers=10,     \n",
    "    iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "output = open(\"eebo-vectors-english.w2v\",\"wb\")\n",
    "gensim.models.Word2Vec.save_word2vec_format(eebo_model,output,binary=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
